{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cc6e8c5",
   "metadata": {},
   "source": [
    "# Predicting customer satisfaction via ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda3ef2e",
   "metadata": {},
   "source": [
    "The client is a startup in the logistics and delivery domain, whose main goal is their customers' satisfaction. Getting feedback from customers is not always easy, but it is the one effective way to gauge customers' satisfaction, and improve their operations accordingly. The company provides us with a subset of a bigger survey, and asked to come up with the most effective ML method to predict customers' happiness from their answers to the survey. \n",
    "\n",
    "In particular, the company asked to:\n",
    "\n",
    "1. create a ML model that is at least 73% accurancy in predicting customer's satisfaction;\n",
    "2. understand which questions are the most crucial to make correct predictions, and which can be removed from the survey without impinging on model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80e1f2c",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c02f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyClassifier\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier # the sklearn GradientBoosting models are reported less performative than the xgboost ones\n",
    "from sklearn.svm import SVC #support vector classifier\n",
    "\n",
    "# accuracy metrics\n",
    "from sklearn.metrics import roc_curve, accuracy_score, classification_report, confusion_matrix, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b78c8",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e714e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ACME-HappinessSurvey2020.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4072c5c6",
   "metadata": {},
   "source": [
    "Data Description:\n",
    "\n",
    "- Y = target attribute (Y) with values indicating 0 (unhappy) and 1 (happy) customers\n",
    "- X1 = my order was delivered on time\n",
    "- X2 = contents of my order was as I expected\n",
    "- X3 = I ordered everything I wanted to order\n",
    "- X4 = I paid a good price for my order\n",
    "- X5 = I am satisfied with my courier\n",
    "- X6 = the app makes ordering easy for me\n",
    "\n",
    "Attributes X1 to X6 indicate the responses for each question and have values from 1 to 5 where the smaller number indicates less and the higher number indicates more towards the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466faa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Y'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f72c25",
   "metadata": {},
   "source": [
    "From the plot above, we can see that there were more happy customers than unhappy customers (though the difference between the two is not as big as one may want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5824d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data, corner=True, kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = data.corr()\n",
    "sns.heatmap(correlations, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74753b9",
   "metadata": {},
   "source": [
    "As we can see from the correlation heatmap above, it looks like customer happiness correlates more with X1 ('my order was delivered on time') and X5 ('I am satisfied with my courier') than with any other feature. This seems to suggest that customery satisfaction hinges on the experience with courier more than anything else (i.e., order content, price, and app experience).\n",
    "\n",
    "Notably, X1 ('my order was delivered on time') highly correlates with and X3 ('I ordered everything I wanted to order'), X5 ('I am satisfied with my courier') and X6 ('the app makes ordering easy for me'). All other combinations had lesser correlation coefficients (< 0.1).\n",
    "\n",
    "Therefore, it looks like X2 ('The content of my order was as I expected') and X4 ('I paid a good price for my order') are not so informative, and may be removed from analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb46296",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412e22b-01ed-471d-9429-5271cbd6ff86",
   "metadata": {},
   "source": [
    "First, let's split the data so that the features *Xn* are separated from the to-be-predicted feature *y* (customer happiness). Then, we split the dataset in two subsets: the _train_ set of *Xn* and *y* features will be used to identify the right ML algorithm to predict customer happiness; the _test_ set will be used to evaluate the chosen algorithm by comparing the predicted *y* values with the true *y* values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2603ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "X = data.drop('Y', axis=1)\n",
    "y = data['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b041fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1) # split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1890385-1b1c-4760-b8b6-ebc7b3f836ad",
   "metadata": {},
   "source": [
    "The `LazyClassifier` class allows us to run all possible estimators, and compare their accuracy scores in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57f00810-5c58-49fd-ad0f-2689bfa341bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 29/29 [00:02<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 58, number of negative: 49\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000194 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 31\n",
      "[LightGBM] [Info] Number of data points in the train set: 107, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.542056 -> initscore=0.168623\n",
      "[LightGBM] [Info] Start training from score 0.168623\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Time Taken</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QuadraticDiscriminantAnalysis</th>\n",
       "      <td>0.63</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBMClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelSpreading</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NuSVC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNeighborsClassifier</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BaggingClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreesClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LabelPropagation</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GaussianNB</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExtraTreeClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NearestCentroid</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Perceptron</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearDiscriminantAnalysis</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifier</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RidgeClassifierCV</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalibratedClassifierCV</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
       "Model                                                                           \n",
       "BernoulliNB                        0.63               0.61     0.61      0.63   \n",
       "QuadraticDiscriminantAnalysis      0.63               0.60     0.60      0.61   \n",
       "LGBMClassifier                     0.58               0.59     0.59      0.58   \n",
       "LabelSpreading                     0.58               0.57     0.57      0.58   \n",
       "NuSVC                              0.58               0.57     0.57      0.58   \n",
       "DummyClassifier                    0.58               0.57     0.57      0.58   \n",
       "AdaBoostClassifier                 0.58               0.55     0.55      0.57   \n",
       "SGDClassifier                      0.53               0.54     0.54      0.53   \n",
       "PassiveAggressiveClassifier        0.58               0.53     0.53      0.54   \n",
       "KNeighborsClassifier               0.58               0.53     0.53      0.54   \n",
       "SVC                                0.58               0.53     0.53      0.54   \n",
       "BaggingClassifier                  0.53               0.52     0.52      0.53   \n",
       "RandomForestClassifier             0.53               0.52     0.52      0.53   \n",
       "ExtraTreesClassifier               0.53               0.52     0.52      0.53   \n",
       "LabelPropagation                   0.53               0.52     0.52      0.53   \n",
       "DecisionTreeClassifier             0.53               0.51     0.51      0.52   \n",
       "GaussianNB                         0.53               0.49     0.49      0.50   \n",
       "ExtraTreeClassifier                0.47               0.48     0.48      0.48   \n",
       "XGBClassifier                      0.47               0.48     0.48      0.48   \n",
       "LogisticRegression                 0.47               0.44     0.44      0.46   \n",
       "NearestCentroid                    0.47               0.44     0.44      0.46   \n",
       "Perceptron                         0.47               0.44     0.44      0.46   \n",
       "LinearDiscriminantAnalysis         0.47               0.44     0.44      0.46   \n",
       "RidgeClassifier                    0.47               0.44     0.44      0.46   \n",
       "RidgeClassifierCV                  0.47               0.44     0.44      0.46   \n",
       "LinearSVC                          0.47               0.44     0.44      0.46   \n",
       "CalibratedClassifierCV             0.42               0.36     0.36      0.34   \n",
       "\n",
       "                               Time Taken  \n",
       "Model                                      \n",
       "BernoulliNB                          0.01  \n",
       "QuadraticDiscriminantAnalysis        0.01  \n",
       "LGBMClassifier                       0.06  \n",
       "LabelSpreading                       0.01  \n",
       "NuSVC                                0.01  \n",
       "DummyClassifier                      0.01  \n",
       "AdaBoostClassifier                   0.08  \n",
       "SGDClassifier                        0.01  \n",
       "PassiveAggressiveClassifier          0.01  \n",
       "KNeighborsClassifier                 0.01  \n",
       "SVC                                  0.01  \n",
       "BaggingClassifier                    0.02  \n",
       "RandomForestClassifier               0.14  \n",
       "ExtraTreesClassifier                 0.08  \n",
       "LabelPropagation                     0.01  \n",
       "DecisionTreeClassifier               0.01  \n",
       "GaussianNB                           0.01  \n",
       "ExtraTreeClassifier                  0.01  \n",
       "XGBClassifier                        0.14  \n",
       "LogisticRegression                   0.01  \n",
       "NearestCentroid                      0.01  \n",
       "Perceptron                           0.01  \n",
       "LinearDiscriminantAnalysis           2.07  \n",
       "RidgeClassifier                      0.01  \n",
       "RidgeClassifierCV                    0.01  \n",
       "LinearSVC                            0.01  \n",
       "CalibratedClassifierCV               0.02  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LazyClassifier(verbose=0, ignore_warnings=True, custom_metric=None)\n",
    "models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd6c11-2155-4a25-ad0a-0cd22a96113a",
   "metadata": {},
   "source": [
    "The most accurate predictions were made by the Decision Tree and the Ada Boost classifiers. However, none of them actually reached our target accuracy threshold (73%). This means that both models are indeed promising in making good predictions for our case, though they might need (i) some hyperparameter tuning (i.e., identification of the best combination of parameters), and/or (ii) some feature selection (i.e., elimination of some features that might be either information, or, more dangerously, noisy, and impede more accurate predictions). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de43bf-fcf6-4204-9ab4-4d92917fd300",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b3e278-cf92-4423-a383-2db2fdc80ae3",
   "metadata": {},
   "source": [
    "We will select only the features that show high correlations coefficients (r >= 0.15) with the target label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23422421-8aa1-4873-963f-a77bb01ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define feature selection\n",
    "features_to_remove = correlations.loc[:, correlations.loc['Y'] < 0.15].columns\n",
    "data_sel = data.drop(features_to_remove, axis=1)\n",
    "\n",
    "X_sel = data_sel.drop('Y', axis=1)\n",
    "y = data_sel['Y']\n",
    "\n",
    "X_selected_train, X_selected_test, y_train, y_test = train_test_split(X_sel, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023c3ed1-bf8e-4ae6-9534-b8b416b5129f",
   "metadata": {},
   "source": [
    "We will proceed to hyperparamenter tuning via GridSearch for each of the following models:\n",
    "1. Logistic regression\n",
    "2. KNN\n",
    "3. XGBoost\n",
    "4. DecisionTree\n",
    "5. Random Forest\n",
    "6. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab7ebfc-80ac-4fe1-aac4-7c60715d45a4",
   "metadata": {},
   "source": [
    "### Hyperparameter Turning via Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e6e984-444b-4635-a1ec-33d572f67e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Report of the model:  LogisticRegression(C=10.0) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.50      0.55         6\n",
      "           1       0.62      0.71      0.67         7\n",
      "\n",
      "    accuracy                           0.62        13\n",
      "   macro avg       0.61      0.61      0.61        13\n",
      "weighted avg       0.61      0.62      0.61        13\n",
      " \n",
      "\n",
      "\n",
      " Report of the model:  DecisionTreeClassifier(max_depth=4) \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.33      0.40         6\n",
      "           1       0.56      0.71      0.63         7\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.53      0.52      0.51        13\n",
      "weighted avg       0.53      0.54      0.52        13\n",
      " \n",
      "\n",
      "\n",
      " Report of the model:  RandomForestClassifier(max_depth=3, min_samples_split=4, n_estimators=30,\n",
      "                       warm_start='False') \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.50      0.50         6\n",
      "           1       0.57      0.57      0.57         7\n",
      "\n",
      "    accuracy                           0.54        13\n",
      "   macro avg       0.54      0.54      0.54        13\n",
      "weighted avg       0.54      0.54      0.54        13\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = ['LogisticRegression', 'DecisionTreeClassifier', 'RandomForestClassifier', 'XGBClassifier', 'KNeighborsClassifier']\n",
    "param_grid = dict().fromkeys(models)\n",
    "\n",
    "param_grid['LogisticRegression'] = {'C': [1/0.0001, 1/0.001, 1/0.01, 1/0.1]} #inverse of regularization: 1/lambda\n",
    "\n",
    "param_grid['DecisionTreeClassifier'] = {'criterion': ['entropy', 'log_loss', 'gini'], # measure of impurity\n",
    "                                        'max_depth': np.arange(1, len(X_selected_train.columns)+1, 1), # how many levels does the tree have?\n",
    "                                        'min_samples_split': [2, 3, 4, 5]} # how many samples are needed to make a split?\n",
    "\n",
    "param_grid['RandomForestClassifier'] = {'n_estimators': [30, 50, 75, 100, 150, 200], # number of trees in the forest\n",
    "                                        'max_depth': np.arange(1, len(X_selected_train.columns)+1, 1), # how many levels does the tree have?\n",
    "                                        'min_samples_split': [2, 3, 4, 5], # how many samples are needed to make a split?\n",
    "                                        'warm_start': ['True', 'False']\n",
    "                                       } \n",
    "param_grid['XGBClassifier'] = {'eta': [0.3, 0.01, 0.001], # i.e., the learning rate\n",
    "                               'max_depth': np.arange(1, len(X_selected_train.columns)+1, 1), # how many levels does the tree have?\n",
    "                               'min_samples_split': [2, 3, 4, 5], # how many samples are needed to make a split?\n",
    "                               'sampling_method': ['uniform', 'gradient_based'], # how are the training data sampled?\n",
    "                               'gamma': [0.5, 1, 1.5, 2, 5], # minimum loss reduction required to make a split -- the larger gamma is, the more conservative the algorithm will be\n",
    "                               'lambda': [0.0001, 0.001, 0.01, 0.1], # regularization term\n",
    "                               'max_leaves': [0, 1, 2, 3, 4, 5]\n",
    "                            }\n",
    "param_grid['KNeighborsClassifier'] = {'n_neighbors' : np.arange(5, 35, 5)}\n",
    "\n",
    "param_grid['SVC'] = {'C': [0.0001, 0.001, 0.01, 0.1], # regularization term\n",
    "                     'kernel' : ['rbf', 'linear']\n",
    "                    }\n",
    "\n",
    "\n",
    "def load_model(model):\n",
    "    if model == 'LogisticRegression':\n",
    "        return LogisticRegression()\n",
    "    if model == \"DecisionTreeClassifier\":\n",
    "        return DecisionTreeClassifier(random_state=42)\n",
    "    if model == 'RandomForestClassifier':\n",
    "        return RandomForestClassifier(random_state=42)\n",
    "    if model == 'XGBClassifier':\n",
    "        return XGBClassifier()\n",
    "    if model == 'KNeighborsClassifier':\n",
    "        return KNeighborsClassifier()\n",
    "    if model =- 'SVC':\n",
    "        return SVC()\n",
    "\n",
    "dict_models = {}\n",
    "for model in models:\n",
    "    estimator = load_model(model)\n",
    "    gs = GridSearchCV(estimator = estimator, param_grid = param_grid[model])\n",
    "    gs.fit(X_selected_train, y_train)\n",
    "    dict_models[model] = gs.best_estimator_\n",
    "    y_pred = dict_models[model].predict(X_selected_test)\n",
    "    print(\"\\n Report of the model: \", dict_models[model],\"\\n\", classification_report(y_test, y_pred),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.4,
   "position": {
    "height": "40px",
    "left": "1368.400024px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
